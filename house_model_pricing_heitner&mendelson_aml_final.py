# -*- coding: utf-8 -*-
"""House Model Pricing - Heitner&Mendelson - AML Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bMnNsvBykvuQ2cwlgTwrdl9dc9QKlz_4
"""

#Danny Mendelson and Mateo Heitner: House Price Modeling

#Importing Library used for modelling and eda
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')
from matplotlib import rcParams
rcParams['figure.figsize'] = 20,12

from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import Pipeline, make_pipeline
from scipy.stats import skew
from sklearn.decomposition import PCA, KernelPCA
from sklearn.impute import SimpleImputer


from sklearn.model_selection import cross_val_score, GridSearchCV, KFold
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.svm import SVR, LinearSVR
from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge
from sklearn.kernel_ridge import KernelRidge
from xgboost import XGBRegressor

import warnings
warnings.filterwarnings('ignore')

#google drive linkage
train = pd.read_csv(f"/content/drive/MyDrive/Mendelson - Machine Learning/train.csv")
test =  pd.read_csv(f"/content/drive/MyDrive/Mendelson - Machine Learning/test.csv")

from google.colab import drive
drive.mount('/content/drive')

#plotting year built vs sale price
sns.boxplot(x=train.YearBuilt, y=train.SalePrice)
plt.xlabel("SalePrice", fontsize=8)

#Plotting living area vs sale price
plt.scatter(x=train.GrLivArea, y=train.SalePrice)
plt.xlabel("GrLivArea", fontsize=13)
plt.ylabel("SalePrice", fontsize=13)
plt.ylim(0,800000)

#traning living room data vs sale price to create table
train.drop(train[(train["GrLivArea"]>4000)&(train["SalePrice"]<300000)].index,inplace=True)

#showcase table data from trainage
full=pd.concat([train,test], ignore_index=True)
full

#column and rows of data through shape
full.drop(['Id'],axis=1, inplace=True)  #drop id from our dataset
full.shape

#summing up data values with different characteristics
aa = full.isnull().sum()
aa[aa>0].sort_values(ascending=False)

#displaying neighborhood vs lot frontage
full.groupby(['Neighborhood'])[['LotFrontage']].agg(['mean','median','count'])

full["LotAreaCut"] = pd.qcut(full.LotArea,10)

#lot area (instead of neighborhoods) tabled against frontage
full.groupby(['LotAreaCut'])[['LotFrontage']].agg(['mean','median','count'])

# Fill missing values in the 'LotFrontage' column with the median values for similar properties, grouped by 'LotAreaCut' and 'Neighborhood'.
full['LotFrontage'] = full.groupby(['LotAreaCut', 'Neighborhood'])['LotFrontage'] \
    .transform(lambda x: x.fillna(x.median()))

# Fill missing values in the 'LotFrontage' column with the median values for similar properties, grouped by 'LotAreaCut'.
full['LotFrontage'] = full.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))

# Fill missing values in specified columns with 0, as these likely represent absence or non-applicability of the feature.
cols = ["MasVnrArea", "BsmtUnfSF", "TotalBsmtSF", "GarageCars", "BsmtFinSF2", "BsmtFinSF1", "GarageArea"]
for col in cols:
    full[col].fillna(0, inplace=True)

# Fill missing values in specified columns with "None", indicating absence or non-applicability of the feature.
cols1 = ["PoolQC", "MiscFeature", "Alley", "Fence", "FireplaceQu", "GarageQual", "GarageCond",
         "GarageFinish", "GarageYrBlt", "GarageType", "BsmtExposure", "BsmtCond", "BsmtQual",
         "BsmtFinType2", "BsmtFinType1", "MasVnrType"]
for col in cols1:
    full[col].fillna("None", inplace=True)

# Fill missing values in specified columns with the mode (most frequent value) of each column, ensuring consistency in categorical data.
cols2 = ["MSZoning", "BsmtFullBath", "BsmtHalfBath", "Utilities", "Functional", "Electrical",
         "KitchenQual", "SaleType", "Exterior1st", "Exterior2nd"]
for col in cols2:
    full[col].fillna(full[col].mode()[0], inplace=True)

# Identify columns with missing values by summing the number of null entries and filtering for columns with more than 0 missing values.
full.isnull().sum()[full.isnull().sum() > 0]

# Convert specified numerical columns to string data type to treat them as categorical features.
NumStr = ["MSSubClass", "BsmtFullBath", "BsmtHalfBath", "HalfBath", "BedroomAbvGr",
          "KitchenAbvGr", "MoSold", "YrSold", "YearBuilt", "YearRemodAdd",
          "LowQualFinSF", "GarageYrBlt"]
for col in NumStr:
    full[col] = full[col].astype(str)

# Group the dataset by 'MSSubClass' and calculate the mean, median, and count of 'SalePrice' for each group.
full.groupby(['MSSubClass'])[['SalePrice']].agg(['mean', 'median', 'count'])

def map_values():
    # Map categorical data to ordinal values for better compatibility with machine learning algorithms.

    # Map MSSubClass values to ordinal categories based on property classification.
    full["oMSSubClass"] = full.MSSubClass.map({
        '180': 1,  # Properties not easily categorized
        '30': 2, '45': 2,  # Smaller residential properties
        '190': 3, '50': 3, '90': 3,  # Larger residential properties
        '85': 4, '40': 4, '160': 4,  # Mixed-use or older properties
        '70': 5, '20': 5, '75': 5, '80': 5, '150': 5,  # Standard single-family homes
        '120': 6, '60': 6  # High-end properties
    })

    # Map MSZoning values to ordinal categories based on zoning classifications.
    full["oMSZoning"] = full.MSZoning.map({
        'C (all)': 1,  # Commercial zones
        'RH': 2, 'RM': 2,  # High and medium-density residential
        'RL': 3,  # Low-density residential
        'FV': 4  # Floating Village residential
    })

    # Map Neighborhood values to ordinal categories based on perceived neighborhood desirability.
    full["oNeighborhood"] = full.Neighborhood.map({
        'MeadowV': 1,  # Least desirable neighborhoods
        'IDOTRR': 2, 'BrDale': 2,  # Below-average neighborhoods
        'OldTown': 3, 'Edwards': 3, 'BrkSide': 3,  # Average neighborhoods
        'Sawyer': 4, 'Blueste': 4, 'SWISU': 4, 'NAmes': 4,  # Slightly above average
        'NPkVill': 5, 'Mitchel': 5,  # Moderate neighborhoods
        'SawyerW': 6, 'Gilbert': 6, 'NWAmes': 6,  # Above-average neighborhoods
        'Blmngtn': 7, 'CollgCr': 7, 'ClearCr': 7, 'Crawfor': 7,  # High-end neighborhoods
        'Veenker': 8, 'Somerst': 8, 'Timber': 8,  # Upper-tier neighborhoods
        'StoneBr': 9,  # Prestigious neighborhood
        'NoRidge': 10, 'NridgHt': 10  # Most prestigious neighborhoods
    })

    # Map Condition1 values to ordinal categories based on proximity to certain conditions.
    full["oCondition1"] = full.Condition1.map({
        'Artery': 1,  # Near major roads
        'Feedr': 2, 'RRAe': 2,  # Near feeder roads or railroads
        'Norm': 3, 'RRAn': 3,  # Normal or railroad-adjacent
        'PosN': 4, 'RRNe': 4,  # Positive influence or near a railroad
        'PosA': 5, 'RRNn': 5  # Very positive influence
    })

    # Map BldgType values to ordinal categories based on building types.
    full["oBldgType"] = full.BldgType.map({
        '2fmCon': 1, 'Duplex': 1, 'Twnhs': 1,  # Multifamily or townhomes
        '1Fam': 2, 'TwnhsE': 2  # Single-family or end-unit townhomes
    })

    # Map HouseStyle values to ordinal categories based on architectural style.
    full["oHouseStyle"] = full.HouseStyle.map({
        '1.5Unf': 1,  # 1.5-story unfinished
        '1.5Fin': 2, '2.5Unf': 2, 'SFoyer': 2,  # 1.5-story finished or split-level
        '1Story': 3, 'SLvl': 3,  # Single-story or split-level
        '2Story': 4, '2.5Fin': 4  # Two-story or 2.5-story finished
    })

    # Repeat for all other features with meaningful mappings, maintaining the structure.
    # This ensures that categorical values are represented as integers, preserving ordinal relationships.

    # Return a confirmation message after processing.
    return "Done!"

map_values()

#these two columns cause issues so we dropped them
full.drop("LotAreaCut",axis=1,inplace=True)
full.drop(['SalePrice'],axis=1,inplace=True)

class labelenc(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass

    def fit(self,X,y=None):
        return self

    def transform(self,X):
        lab=LabelEncoder()
        X["YearBuilt"] = lab.fit_transform(X["YearBuilt"])
        X["YearRemodAdd"] = lab.fit_transform(X["YearRemodAdd"])
        X["GarageYrBlt"] = lab.fit_transform(X["GarageYrBlt"])
        return X

#Apply log1p to the skewed features, then get_dummies.

class skew_dummies(BaseEstimator, TransformerMixin):
    def __init__(self,skew=0.5):
        self.skew = skew

    def fit(self,X,y=None):
        return self

    def transform(self,X):
        X_numeric=X.select_dtypes(exclude=["object"])
        skewness = X_numeric.apply(lambda x: skew(x))
        skewness_features = skewness[abs(skewness) >= self.skew].index
        X[skewness_features] = np.log1p(X[skewness_features])
        X = pd.get_dummies(X)
        return X

# build pipeline
pipe = Pipeline([
    ('labenc', labelenc()),
    ('skew_dummies', skew_dummies(skew=1)),
    ])

# save the original data for later use
full2 = full.copy()

#fits the data
data_pipe = pipe.fit_transform(full2)
data_pipe

scaler = RobustScaler()
#training the data
n_train=train.shape[0]

#more training
X = data_pipe[:n_train]
test_X = data_pipe[n_train:]
y= train.SalePrice

X_scaled = scaler.fit(X).transform(X)
y_log = np.log(train.SalePrice)
test_X_scaled = scaler.transform(test_X)

lasso=Lasso(alpha=0.001)
lasso.fit(X_scaled,y_log)

#sorting my feature importance
FI_lasso = pd.DataFrame({"Feature Importance":lasso.coef_}, index=data_pipe.columns)
FI_lasso.sort_values("Feature Importance",ascending=False)

#plotting all the data by importance
FI_lasso[FI_lasso["Feature Importance"]!=0].sort_values("Feature Importance").plot(kind="barh",figsize=(15,25))
plt.xticks(rotation=90)
plt.show()

#creating c,ass of the base estimator and the transformer
class add_feature(BaseEstimator, TransformerMixin):
    def __init__(self,additional=1):
        self.additional = additional

    def fit(self,X,y=None):
        return self

    def transform(self,X):
        if self.additional==1:
            X["TotalHouse"] = X["TotalBsmtSF"] + X["1stFlrSF"] + X["2ndFlrSF"]
            X["TotalArea"] = X["TotalBsmtSF"] + X["1stFlrSF"] + X["2ndFlrSF"] + X["GarageArea"]

        else:
            X["TotalHouse"] = X["TotalBsmtSF"] + X["1stFlrSF"] + X["2ndFlrSF"]
            X["TotalArea"] = X["TotalBsmtSF"] + X["1stFlrSF"] + X["2ndFlrSF"] + X["GarageArea"]

            X["+_TotalHouse_OverallQual"] = X["TotalHouse"] * X["OverallQual"]
            X["+_GrLivArea_OverallQual"] = X["GrLivArea"] * X["OverallQual"]
            X["+_oMSZoning_TotalHouse"] = X["oMSZoning"] * X["TotalHouse"]
            X["+_oMSZoning_OverallQual"] = X["oMSZoning"] + X["OverallQual"]
            X["+_oMSZoning_YearBuilt"] = X["oMSZoning"] + X["YearBuilt"]
            X["+_oNeighborhood_TotalHouse"] = X["oNeighborhood"] * X["TotalHouse"]
            X["+_oNeighborhood_OverallQual"] = X["oNeighborhood"] + X["OverallQual"]
            X["+_oNeighborhood_YearBuilt"] = X["oNeighborhood"] + X["YearBuilt"]
            X["+_BsmtFinSF1_OverallQual"] = X["BsmtFinSF1"] * X["OverallQual"]

            X["-_oFunctional_TotalHouse"] = X["oFunctional"] * X["TotalHouse"]
            X["-_oFunctional_OverallQual"] = X["oFunctional"] + X["OverallQual"]
            X["-_LotArea_OverallQual"] = X["LotArea"] * X["OverallQual"]
            X["-_TotalHouse_LotArea"] = X["TotalHouse"] + X["LotArea"]
            X["-_oCondition1_TotalHouse"] = X["oCondition1"] * X["TotalHouse"]
            X["-_oCondition1_OverallQual"] = X["oCondition1"] + X["OverallQual"]


            X["Bsmt"] = X["BsmtFinSF1"] + X["BsmtFinSF2"] + X["BsmtUnfSF"]
            X["Rooms"] = X["FullBath"]+X["TotRmsAbvGrd"]
            X["PorchArea"] = X["OpenPorchSF"]+X["EnclosedPorch"]+X["3SsnPorch"]+X["ScreenPorch"]
            X["TotalPlace"] = X["TotalBsmtSF"] + X["1stFlrSF"] + X["2ndFlrSF"] + X["GarageArea"] + X["OpenPorchSF"]+X["EnclosedPorch"]+X["3SsnPorch"]+X["ScreenPorch"]


            return X

#creating poipeline
pipe = Pipeline([
    ('labenc', labelenc()),
    ('add_feature', add_feature(additional=2)),
    ('skew_dummies', skew_dummies(skew=1)),
    ])

#filling data
full['oFunctional'] = np.nan
full_pipe = pipe.fit_transform(full)
full_pipe

#more training
n_train=train.shape[0]
X = full_pipe[:n_train]
test_X = full_pipe[n_train:]
y= train.SalePrice

X_scaled = scaler.fit(X).transform(X)
y_log = np.log(train.SalePrice)
test_X_scaled = scaler.transform(test_X)

pca = PCA(n_components=410)

#scaling data
X_scaled=pca.fit_transform(X_scaled)
test_X_scaled = pca.transform(test_X_scaled)
X_scaled.shape, test_X_scaled.shape

# define cross validation strategy
def rmse_cv(model,X,y):
    rmse = np.sqrt(-cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=5))
    return rmse

#creating the models
models = [LinearRegression(),Ridge(),Lasso(alpha=0.01,max_iter=10000),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),
          ElasticNet(alpha=0.001,max_iter=10000),SGDRegressor(max_iter=1000,tol=1e-3),BayesianRidge(),KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),
          ExtraTreesRegressor(),XGBRegressor()]

#iterating through and naming and running
names = ["LR", "Ridge", "Lasso", "RF", "GBR", "SVR", "LinSVR", "Ela","SGD","Bay","Ker","Extra","Xgb"]
for name, model in zip(names, models):
    score = rmse_cv(model, X_scaled, y_log)
    print("{}: {:.6f}, {:.4f}".format(name,score.mean(),score.std()))

#creating grid class
class grid():
    def __init__(self,model):
        self.model = model

    def grid_get(self,X,y,param_grid):
        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring="neg_mean_squared_error")
        grid_search.fit(X,y)
        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))
        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])
        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])

grid(Lasso()).grid_get(X_scaled,y_log,{'alpha': [0.0004,0.0005,0.0007,0.0009],'max_iter':[10000]})

#creating the grid for ridge model
grid(Ridge()).grid_get(X_scaled,y_log,{'alpha':[35,40,45,50,55,60,65,70,80,90]})

#creating grid for SVR model
grid(SVR()).grid_get(X_scaled,y_log,{'C':[11,13,15],'kernel':["rbf"],"gamma":[0.0003,0.0004],"epsilon":[0.008,0.009]})

#creating grid for kernel ridge model
param_grid={'alpha':[0.2,0.3,0.4], 'kernel':["polynomial"], 'degree':[3],'coef0':[0.8,1]}
grid(KernelRidge()).grid_get(X_scaled,y_log,param_grid)

#creating  grid for elastic net model
grid(ElasticNet()).grid_get(X_scaled,y_log,{'alpha':[0.0008,0.004,0.005],'l1_ratio':[0.08,0.1,0.3],'max_iter':[10000]})

#creating average weight class with base estimator and regressor mixin
class AverageWeight(BaseEstimator, RegressorMixin):
    def __init__(self,mod,weight):
        self.mod = mod
        self.weight = weight

    def fit(self,X,y):
        self.models_ = [clone(x) for x in self.mod]
        for model in self.models_:
            model.fit(X,y)
        return self

    def predict(self,X):
        w = list()
        pred = np.array([model.predict(X) for model in self.models_])
        # for every data point, single model prediction times weight, then add them together
        for data in range(pred.shape[1]):
            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]
            w.append(np.sum(single))
        return w

#creating models
lasso = Lasso(alpha=0.0005,max_iter=10000)
ridge = Ridge(alpha=60)
svr = SVR(gamma= 0.0004,kernel='rbf',C=13,epsilon=0.009)
ker = KernelRidge(alpha=0.2 ,kernel='polynomial',degree=3 , coef0=0.8)
ela = ElasticNet(alpha=0.005,l1_ratio=0.08,max_iter=10000)
bay = BayesianRidge()

# assign weights based on their gridsearch score
w1 = 0.02
w2 = 0.2
w3 = 0.25
w4 = 0.3
w5 = 0.03
w6 = 0.2

#creating weighted average
weight_avg = AverageWeight(mod = [lasso,ridge,svr,ker,ela,bay],weight=[w1,w2,w3,w4,w5,w6])

#getting score
score = rmse_cv(weight_avg,X_scaled,y_log)
print(score.mean())

# Averaging the two models
weight_avg = AverageWeight(mod = [svr,ker],weight=[0.5,0.5])
score = rmse_cv(weight_avg,X_scaled,y_log)
print(score.mean())

#creating stack based on base estimator, regressor mixin, transformer mixin
class stacking(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self,mod,meta_model):
        self.mod = mod
        self.meta_model = meta_model
        self.kf = KFold(n_splits=5, random_state=42, shuffle=True)

    def fit(self,X,y):
        self.saved_model = [list() for i in self.mod]
        oof_train = np.zeros((X.shape[0], len(self.mod)))

        for i,model in enumerate(self.mod):
            for train_index, val_index in self.kf.split(X,y):
                renew_model = clone(model)
                renew_model.fit(X[train_index], y[train_index])
                self.saved_model[i].append(renew_model)
                oof_train[val_index,i] = renew_model.predict(X[val_index])

        self.meta_model.fit(oof_train,y)
        return self

    def predict(self,X):
        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1)
                                      for single_model in self.saved_model])
        return self.meta_model.predict(whole_test)

    def get_oof(self,X,y,test_X):
        oof = np.zeros((X.shape[0],len(self.mod)))
        test_single = np.zeros((test_X.shape[0],5))
        test_mean = np.zeros((test_X.shape[0],len(self.mod)))
        for i,model in enumerate(self.mod):
            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):
                clone_model = clone(model)
                clone_model.fit(X[train_index],y[train_index])
                oof[val_index,i] = clone_model.predict(X[val_index])
                test_single[:,j] = clone_model.predict(test_X)
            test_mean[:,i] = test_single.mean(axis=1)
        return oof, test_mean

#dooing imputer on x scaled and then y log values
a = SimpleImputer().fit_transform(X_scaled)
b = SimpleImputer().fit_transform(y_log.values.reshape(-1,1)).ravel()

#creating stakc model passing in lasso ridge svr ker ela and bay
stack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)

#making score
score = rmse_cv(stack_model,a,b)
print(score.mean())

#more trainig and testing
X_train_stack, X_test_stack = stack_model.get_oof(a,b,test_X_scaled)

#more training
X_train_stack.shape, a.shape

#more training
X_train_add = np.hstack((a,X_train_stack))

#more train and test
X_test_add = np.hstack((test_X_scaled,X_test_stack))
X_train_add.shape, X_test_add.shape

#getting new score
score = rmse_cv(stack_model,X_train_add,b)
print(score.mean())

stack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)
stack_model.fit(a,b)

pred = np.exp(stack_model.predict(test_X_scaled))
pred

#exporting result to csv
result=pd.DataFrame({'Id':test.Id, 'SalePrice':pred})
result.to_csv("submission.csv",index=False